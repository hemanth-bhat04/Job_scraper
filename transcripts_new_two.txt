a simple approach to classification is using what's called a naive bayes bottle actually it's a family of models it's a model that make uses the the so called naive bayes assumption and let me um point out here first even though has this the word bayes in it it's not necessarily a bayesian method so if you if you've heard about bayesian methods this is not necessarily a bayesian method but you can do bayesian naive bayes so the nice thing about naive bayes is that it's simple and it can work it can work okay i can work i mean it depends on the problem not as good as there are other methods which are much better better performance but the nice thing about naive basis is very simple to understand easy to implement and it's a good it's a good way to describe or it's a good introduction to generative classification models and illustrates some a lot of the interesting points methods associated with this type of model so what is naive bayes well here's the set up it's a classification problem so it's a supervised learning problem and so we're given some data some pairs of x's and y's x y up to xn y and actually so let me i'm going to use a little different notation here instead of putting subscripts for the points i'm going to use super scripts and you'll see why i'm doing that in a minute just for to keep things unambiguous so here each of these x is is a point in rd so it's the first coordinate i will write this way the superscript will always be in parentheses and that will tell you which point it is the subscript tells you which coordinate so it's this is a point in our d d dimensional real space and the y's so i'll just i'll just say that here so x i is a point in our d and i writing it writing it all out in this way and why i is oh no it's not a real number so y is in some finite set we're doing classification this is the classification method and y is just some so here y this capital script y is some finite set let's say maybe just the integers from to m just some finite set so that's what we're given and we will assume under the naive bayes approach we assume a probabilistic model in the following way so we assume a family that's some set of distributions a family of distributions parametrized by some parameter theta could be a vector just just some some parameter here and these distributions will have the following the following property this is the key the key property so each of these is a distribution it's a joint distribution so this is a naive bayes is a generative model so in contrast to the discriminative model we're going to assume a joint distribution on x and y so here this x x is going to be one of these x's so it's at point n rd and y is a point in r it's a class and the joint probability of these under the parameter theta well of course we can we can always factor it as the probability of x given y times the probability of y that's we can always do that but here is the key assumption this is the key assumption in the naive bayes model here it comes so we assume that this part here factors as the product of the probability of the first coordinate given y times the probability of the second coordinate given y and so on up to the probability
of the d coordinate given y so this is this is the key assumption here okay and so if we have some random variables let's say so we assume this family of distributions and now we're going to model our data so we'll let x superscript let's see how do i want to say this yeah let's say this one y once we have these pairs up to xn yn we assume that these are distributed according to this distant distribution p theta and there iid for some you know so we assume that there's some theta for which these random variables are ai are randomly distributed and then we're going to assume that our data comes from such random variables so this assumption here this the assumption of the this the probability of x given y factors in this way this is the same as saying that so x was so if x we put that in a different color now we'll keep the same color so this assumption says that if xy is distributed according to p theta then the coordinates x up to x d of this random x are independent given y so there sometimes we say conditionally independent to emphasize that it's given they're independent given y so this assumption is a this is a conditional independence conditional independence assumption so that is and that is the what is the that's the naive part of the model so this this conditional independence assumption is the naive part that's what makes it you know so called naive i mean it's it's um it's a reasonable assumption there's nothing wrong with this assumption it's just some probabilistic model but this is what people call it for better for worse a better name might be a independent feature model that might be a better name independent feature model so these these different coordinates x through x b can be thought of as or you could be just referred to as features the features of this x ok so that's our assumption that's that the model that we assumed and now how are we going to use this model well we need to do classification with this that's what we're trying to do is try to view classification so our goal is for a new x we get some new x in our d classify or i'll say predict it's y so we want to predict the class for that x and under this model how will we do that well we will do the following so the algorithm the algorithm that we will use is first this is the standard thing to do first we estimate theta so our assumption here we assume that there's this family and that our data comes from some random variables that are distributed according to that model for some theta but we don't know the truth data so this is the classic estimation type of problem we have some data we want to estimate the parameter
for that data so this you know we know sort of how to do what we've seen ways how to do this you could use maximum likelihood you could use map you could use the maximum a-posteriori so there's so this is something that we we know how to do so we're going to estimate theta from the data d and then we will we will compute so we want to compute y hat our prediction we want to maximize the probability so we want to choose a prediction that maximizes over all possible classes the probability of that class given this new x here under our estimated theta so maybe i should put like theta hat here to emphasize that that's for our that this is using the estimate that we that we we made in the first step so we want to we want to compute this and that's well okay so that i mean we could stop right there and that would be this would fully define the naive bayes procedure you compute this thing this is a this is the defined quantity under this model and and that's it that's the naive bayes that's a naive bayes classifier i'd like to go a little bit further here and expand this expression a little bit so that you see exactly how this model that we've assumed how it allows you to compute this so illuminating so this thing so let's we'll just continue this line here this will keep the arg max over y so let's use bayes rule  remember that bayes rule tells us this is the probability of x given y times probability of y divided by the probability of x that's just bayes rule and now note that the denominator here does not depend on y and since we're maximizing over y it's the same thing you know we could just forget about the not denominator here and just maximize this thing so that's say in other words i'm saying that this as a function of y is proportional to to this part and therefore this is we have this equality the arg max of this or an arg max of this as a narc max of this so i guess technically i should but you know we wanna a y hat that's one of the maximizers it's not necessarily since it's not necessarily a unique maximizer so yeah to be precise i should have i should have put this and this is considering the arg max to be the set of maximizes okay so we we did this little thing here and now right our assumption is that this part our bottle where's our model here's a model our model says that this this thing fact or this part factors in this way so we can do that let's plug that in our max over y x given y so it's the probability of each of the coordinates given y times the probability of y and this this part this step here where i used bayes rule this is the the bayes part of the algorithm so it's not you know bayesian in the sense that we're we're putting a prior on parameters and integrating out that parameter but this is this is why they call it naive bayes in particular so we can compute this thing we expand it out and we use our our estimated theta to compute these and we can we can maximize it and we get our prediction and so that is the naive bayes classifier
